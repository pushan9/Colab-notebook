{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushan9/Colab-notebook/blob/main/Text_Generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CHNX4Tn4cPb1",
      "metadata": {
        "id": "CHNX4Tn4cPb1"
      },
      "source": [
        "# **Demo: Text Generation**\n",
        "\n",
        "This demonstration employs the Natural Language Toolkit (NLTK) and the Brown corpus to demonstrate text generation through a Markov chain model using trigrams."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k-2DSxXRc1gn",
      "metadata": {
        "id": "k-2DSxXRc1gn"
      },
      "source": [
        "## **Steps to Perform:**\n",
        "\n",
        "Step 1: Import the Necessary Libraries\n",
        "\n",
        "Step 2: Define Stopwords and Punctuation\n",
        "\n",
        "Step 3: Load Sentences and Generate N-grams\n",
        "\n",
        "Step 4: Remove Stopwords from N-grams\n",
        "\n",
        "Step 5: Calculate Frequency Distributions\n",
        "\n",
        "Step 6: Create a Dictionary of Trigram Frequencies\n",
        "\n",
        "Step 7: Define the Text Generation Function\n",
        "\n",
        "Step 8: Execute the Text Generation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F77eQldddrgN",
      "metadata": {
        "id": "F77eQldddrgN"
      },
      "source": [
        "###**Step 1: Import the Necessary Libraries**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "*   Import the necessary libraries and set up the OpenAI API key.\n",
        "*   Download the necessary NLTK packages and corpus.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PgPYiaZsc-3O",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgPYiaZsc-3O",
        "outputId": "4b77d5c3-4867-460f-e503-c46b4978fe52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "import string # string.punctuation → list of punctuation characters (.,?!;:). Useful for cleaning text before generating (removing unwanted symbols).\n",
        "\n",
        "import random # For random sampling — selecting a random word or next token based on probability.\n",
        "\n",
        "import nltk # one of the oldest and most widely used NLP libraries in Python.\n",
        "\n",
        "from nltk import FreqDist # a class for counting word frequencies.\n",
        "\n",
        "from nltk.corpus import brown # Brown Corpus, one of the first large, balanced English text corpora.\n",
        "# The Brown corpus contains around a million words from diverse sources — news, fiction, essays, etc.\n",
        "\n",
        "from collections import defaultdict, Counter # defaultdict → dictionary with a default value for missing keys (avoids key errors).\n",
        "\n",
        "from nltk.util import ngrams\n",
        "# Automatically creates n-grams (groups of consecutive words):\n",
        "# n=2 → bigrams (“machine learning”)\n",
        "# n=3 → trigrams (“deep learning model”)\n",
        "# eg: I am a good boy → bigrams: (I am), (am a), (a good), (good boy)\n",
        "# eg: I am a good boy → trigrams: (I am a), (am a good), (a good boy)\n",
        "\n",
        "# Download necessary NLTK packages and corpus\n",
        "nltk.download('punkt') # Tokenizer models for splitting text into words/sentences.\n",
        "nltk.download('stopwords') # Common words (the, is, in) to ignore during text processing.\n",
        "nltk.download('brown') # Brown Corpus for training language models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QuDAtVJAeKPZ",
      "metadata": {
        "id": "QuDAtVJAeKPZ"
      },
      "source": [
        "### **Step 2: Define Stopwords and Punctuation**\n",
        "\n",
        "*   Stopwords are common words in a language that are often considered to be of little value in text analysis.\n",
        "*   Punctuation refers to characters used to separate sentences, clauses, phrases, or words in writing.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2MHWS4tUeZYO",
      "metadata": {
        "id": "2MHWS4tUeZYO",
        "outputId": "7a5c0ecd-4e06-40b5-e116-4ad3a3b396b8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['the', 'technology', 'advances', '—', 'lt', 'rt']"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "# Define stopwords and punctuation\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "string.punctuation += '\"\\'-—' # Adds extra punctuation characters (\", ', -, —) to Python’s built-in list of punctuation. eg., to remove quotes and dashes from text - \"it's\", \"well-known\", \"—he said—\"\n",
        "\n",
        "removal_list = list(stop_words) + list(string.punctuation) + ['lt', 'rt']\n",
        "# These are HTML-like or Twitter-style tokens:\n",
        "# lt = “less than” (from <)\n",
        "# rt = “retweet”\n",
        "# Such tokens appear in older or web-scraped corpora (like social media or Brown samples).\n",
        "# They are not meaningful words, so they are removed too.\n",
        "# When you build your n-gram model later, you’ll feed it cleaned text like:\n",
        "# [\"technology\", \"advances\", \"rapidly\"]\n",
        "# instead of messy text like:\n",
        "[\"the\", \"technology\", \"advances\", \"—\", \"lt\", \"rt\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BD7fr4rheeOW",
      "metadata": {
        "id": "BD7fr4rheeOW"
      },
      "source": [
        "### **Step 3: Load Sentences and Generate N-grams**\n",
        "\n",
        "*   Load sentences from the Brown corpus and generate N-grams.\n",
        "*   By the end of this process, **unigram**, **bigram**, and **trigram** lists will contain the respective N-grams for the sentences in the Brown corpus.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "697d9365",
      "metadata": {
        "id": "697d9365",
        "outputId": "f344cad9-1fd7-4ac9-fc5f-c3ea663dfcee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 2 sentences from Brown Corpus:\n",
            "The Fulton County Grand Jury said Friday an investigation of Atlanta's recent primary election produced `` no evidence '' that any irregularities took place .\n",
            "The jury further said in term-end presentments that the City Executive Committee , which had over-all charge of the election , `` deserves the praise and thanks of the City of Atlanta '' for the manner in which the election was conducted .\n",
            "\n",
            "Total sentences in Brown Corpus: 57340\n"
          ]
        }
      ],
      "source": [
        "# Load sentences from the Brown corpus\n",
        "sents = brown.sents()\n",
        "# Loads all sentences from the Brown corpus.\n",
        "# Each sentence is returned as a list of words, e.g.\n",
        "# [\"The\", \"jury\", \"said\", \"it\", \"found\", \"this\", \"decision\", \"unfair\"].\n",
        "\n",
        "# Lets's print the first 2 sentences to see how they look\n",
        "print(\"First 2 sentences from Brown Corpus:\")\n",
        "for i in range(2):\n",
        "    print(' '.join(sents[i]))\n",
        "\n",
        "# Print the total number of sentences in the Brown corpus\n",
        "print(f\"\\nTotal sentences in Brown Corpus: {len(sents)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pTKPSbw_elM2",
      "metadata": {
        "id": "pTKPSbw_elM2"
      },
      "outputs": [],
      "source": [
        "# Initialize lists for storing n-grams\n",
        "# Creates three empty lists to hold:\n",
        "unigram = [] # single words\n",
        "bigram = [] # pairs of consecutive words\n",
        "trigram = [] # triplets of consecutive words\n",
        "\n",
        "# Generate n-grams\n",
        "for sentence in sents:\n",
        "    sentence = [word.lower() for word in sentence if word not in string.punctuation] # Cleans each sentence by converting words to lowercase and removing punctuation.\n",
        "    unigram.extend(sentence) # Adds single words to the unigram list.\n",
        "    bigram.extend(list(ngrams(sentence, 2, pad_left=True, pad_right=True))) # pad_left=True and pad_right=True add special None tokens at sentence boundaries so you can model sentence beginnings and endings.\n",
        "    # Example:\n",
        "    # Sentence = [\"the\",\"cat\",\"sat\"] →\n",
        "    # Bigrams = [(None,\"the\"), (\"the\",\"cat\"), (\"cat\",\"sat\"), (\"sat\",None)]\n",
        "\n",
        "    trigram.extend(list(ngrams(sentence, 3, pad_left=True, pad_right=True)))\n",
        "    # Example:\n",
        "    # Sentence = [\"the\",\"cat\",\"sat\"] →\n",
        "    # Trigrams = [(None,None,\"the\"), (None,\"the\",\"cat\"), (\"the\",\"cat\",\"sat\"), (\"cat\",\"sat\",None), (\"sat\",None,None)]."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2109306e",
      "metadata": {
        "id": "2109306e",
        "outputId": "41975988-23c0-4914-a1c8-8eb4bf09c7db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Unigrams: ['thunder', 'drag', 'persisted', 'carried', 'pope', 'with', 'bill', 'night', 'in', 'was']\n",
            "\n",
            "Sample Bigrams: [('also', 'will'), ('not', 'exceed'), ('new', 'as'), ('all', 'questions'), ('her', 'unusual'), ('and', 'all'), ('differences', 'the'), (\"ingleside's\", 'drunk-and-disorderlies'), ('and', '``'), ('it', 'by')]\n",
            "\n",
            "Sample Trigrams: [('eyes', 'of', 'all'), ('as', 'a', 'result'), ('as', 'an', 'air'), ('of', 'these', 'benefits'), ('few', 'communities', 'that'), (None, None, 'the'), ('new', 'york', None), ('other', 'group', 'of'), ('robert', 'zeising', 'mrs.'), ('at', 'about', '600-degrees')]\n"
          ]
        }
      ],
      "source": [
        "# Lets print a few samples of each n-gram type\n",
        "print(\"\\nSample Unigrams:\", random.sample(unigram, 10))\n",
        "print(\"\\nSample Bigrams:\", random.sample(bigram, 10))\n",
        "print(\"\\nSample Trigrams:\", random.sample(trigram, 10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "25c84f20",
      "metadata": {
        "id": "25c84f20",
        "outputId": "8a390246-f2b5-455e-f746-31bf7b23befa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 10 Bigrams with padding: [(None, 'the'), ('the', 'fulton'), ('fulton', 'county'), ('county', 'grand'), ('grand', 'jury'), ('jury', 'said'), ('said', 'friday'), ('friday', 'an'), ('an', 'investigation'), ('investigation', 'of')]\n",
            "\n",
            "Last 10 Bigrams with padding: [('glance', 'the'), ('the', 'figure'), ('figure', 'inside'), ('inside', 'the'), ('the', 'coral-colored'), ('coral-colored', 'boucle'), ('boucle', 'dress'), ('dress', 'was'), ('was', 'stupefying'), ('stupefying', None)]\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 bigrams to verify padding\n",
        "print(\"\\nFirst 10 Bigrams with padding:\", bigram[:10])\n",
        "# Print the last 10 bigrams to verify padding\n",
        "print(\"\\nLast 10 Bigrams with padding:\", bigram[-10:])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04a0378d",
      "metadata": {
        "id": "04a0378d",
        "outputId": "669f476d-1b5c-453a-e8f6-e240a0c426c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "First 10 Trigrams with padding: [(None, None, 'the'), (None, 'the', 'fulton'), ('the', 'fulton', 'county'), ('fulton', 'county', 'grand'), ('county', 'grand', 'jury'), ('grand', 'jury', 'said'), ('jury', 'said', 'friday'), ('said', 'friday', 'an'), ('friday', 'an', 'investigation'), ('an', 'investigation', 'of')]\n",
            "\n",
            "Last 10 Trigrams with padding: [('glance', 'the', 'figure'), ('the', 'figure', 'inside'), ('figure', 'inside', 'the'), ('inside', 'the', 'coral-colored'), ('the', 'coral-colored', 'boucle'), ('coral-colored', 'boucle', 'dress'), ('boucle', 'dress', 'was'), ('dress', 'was', 'stupefying'), ('was', 'stupefying', None), ('stupefying', None, None)]\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 trigrams to verify padding\n",
        "print(\"\\nFirst 10 Trigrams with padding:\", trigram[:10])\n",
        "# Print the last 10 trigrams to verify padding\n",
        "print(\"\\nLast 10 Trigrams with padding:\", trigram[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oXbe4Qgpepzk",
      "metadata": {
        "id": "oXbe4Qgpepzk"
      },
      "source": [
        "### **Step 4: Remove Stopwords from N-grams**\n",
        "\n",
        "*   Define a function to remove stopwords from the N-grams.\n",
        "*   Use it to clean the bigrams and trigrams.\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0a344df4",
      "metadata": {
        "id": "0a344df4",
        "outputId": "3c486daa-a16d-4e8a-a9f7-a878324545a4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Removal List Sample: ['theirs', 'than', 'herself', 'where', 'all', 'at', 'those', 'does', 'ain', 'there']\n"
          ]
        }
      ],
      "source": [
        "# Print the first 10 items in removal_list\n",
        "print(\"\\nRemoval List Sample:\", removal_list[:10])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6VHX-QXhe86T",
      "metadata": {
        "id": "6VHX-QXhe86T"
      },
      "outputs": [],
      "source": [
        "# Function to remove stopwords from n-grams\n",
        "def remove_stopwords(ngrams, n):\n",
        "    if n == 2:\n",
        "        return [(a, b) for (a, b) in ngrams if a not in removal_list and b not in removal_list]\n",
        "    elif n == 3:\n",
        "        return [(a, b, c) for (a, b, c) in ngrams if a not in removal_list and b not in removal_list and c not in removal_list]\n",
        "\n",
        "# Remove stopwords from n-grams\n",
        "bigram = remove_stopwords(bigram, 2)\n",
        "trigram = remove_stopwords(trigram, 3)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5871edb4",
      "metadata": {
        "id": "5871edb4",
        "outputId": "c6862829-5880-448e-d450-9a2976d6aca1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Cleaned Bigrams: [('utopia', None), ('vandals', 'naught'), ('guided', 'exposure'), ('small', 'matter'), ('quiet', 'moment'), ('plastics', 'units'), (\"watson-watt's\", 'remarks'), ('quickly', 'away'), ('md.', 'march'), ('gratitude', None)]\n",
            "\n",
            "Sample Cleaned Trigrams: [(None, None, 'among'), (None, None, '``'), ('three', 'oranges', 'gay'), ('screeching', 'war', 'whoop'), (None, None, 'fresh'), ('instead', 'met', 'violent'), ('find', \"kayabashi's\", 'secretary'), ('ap', None, None), ('sizzling', 'day', None), (None, None, 'issue')]\n"
          ]
        }
      ],
      "source": [
        "# Lets print a few samples of cleaned n-grams\n",
        "print(\"\\nSample Cleaned Bigrams:\", random.sample(bigram, 10))\n",
        "print(\"\\nSample Cleaned Trigrams:\", random.sample(trigram, 10))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "FLLxqiIufGp-",
      "metadata": {
        "id": "FLLxqiIufGp-"
      },
      "source": [
        "###**Step 5: Calculate Frequency Distributions**\n",
        "\n",
        "*   Calculate the frequency distributions of the bigrams and trigrams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Zmq_1nIWfNg4",
      "metadata": {
        "id": "Zmq_1nIWfNg4"
      },
      "outputs": [],
      "source": [
        "# Calculate frequency distributions\n",
        "# FreqDist() is a built-in NLTK class that counts how many times each unique item appears in a list.\n",
        "freq_bi = FreqDist(bigram)\n",
        "freq_tri = FreqDist(trigram)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2fdf0ef",
      "metadata": {
        "id": "c2fdf0ef",
        "outputId": "6a0f3f2c-6275-4c96-a01b-00d58fdbd877",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most Common Bigrams: [((\"''\", None), 4747), ((None, '``'), 4177), (('said', None), 445), ((None, 'one'), 401), (('united', 'states'), 392), (('said', '``'), 323), (('new', 'york'), 296), ((None, 'mr.'), 241), (('af', None), 236), ((None, '--'), 219)]\n",
            "\n",
            "Most Common Trigrams: [((\"''\", None, None), 4747), ((None, None, '``'), 4177), (('said', None, None), 445), ((None, None, 'one'), 401), ((None, None, None), 242), ((None, None, 'mr.'), 241), (('af', None, None), 236), ((None, None, '--'), 219), (('time', None, None), 205), ((None, None, 'even'), 190)]\n",
            "\n",
            "\n",
            "Least Common Bigrams: [((\"novelist's\", 'carping'), 1), (('carping', 'phrase'), 1), (('lower', 'lip'), 1), (('voluptuous', None), 1), (('swift', 'greedy'), 1), (('greedy', 'glance'), 1), (('figure', 'inside'), 1), (('coral-colored', 'boucle'), 1), (('boucle', 'dress'), 1), (('stupefying', None), 1)]\n",
            "\n",
            "Least Common Trigrams: [(('j.', 'perelman', None), 1), (('perelman', None, None), 1), ((None, None, 'revulsion'), 1), (('train', 'slid', 'shut'), 1), (('wide', 'cheekbones', 'olive-flushed'), 1), ((\"novelist's\", 'carping', 'phrase'), 1), (('voluptuous', None, None), 1), (('swift', 'greedy', 'glance'), 1), (('coral-colored', 'boucle', 'dress'), 1), (('stupefying', None, None), 1)]\n"
          ]
        }
      ],
      "source": [
        "# Lets print a sample of cleaned bigrams and trigrams\n",
        "print(\"\\nMost Common Bigrams:\", freq_bi.most_common(10))\n",
        "print(\"\\nMost Common Trigrams:\", freq_tri.most_common(10))\n",
        "print()\n",
        "# Lets also print least common n-grams to see rare combinations\n",
        "print(\"\\nLeast Common Bigrams:\", freq_bi.most_common()[-10:])\n",
        "print(\"\\nLeast Common Trigrams:\", freq_tri.most_common()[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sB-t5lnjfP_V",
      "metadata": {
        "id": "sB-t5lnjfP_V"
      },
      "source": [
        "### **Step 6: Create a Dictionary of Trigram Frequencies**\n",
        "\n",
        "*   Create a dictionary of trigram frequencies to use it in the text generation function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fd045ec",
      "metadata": {
        "id": "0fd045ec",
        "outputId": "5af30534-b248-4c98-837a-a9003c072fef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Most Common Trigrams: [((\"''\", None, None), 4747), ((None, None, '``'), 4177), (('said', None, None), 445), ((None, None, 'one'), 401), ((None, None, None), 242), ((None, None, 'mr.'), 241), (('af', None, None), 236), ((None, None, '--'), 219), (('time', None, None), 205), ((None, None, 'even'), 190)]\n"
          ]
        }
      ],
      "source": [
        "print(\"\\nMost Common Trigrams:\", freq_tri.most_common(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DyEVQsOxfaUq",
      "metadata": {
        "id": "DyEVQsOxfaUq",
        "outputId": "83e9154c-c23b-49a5-e94a-d9495d8d1c55",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Sample Trigram Frequency Dictionary Entries:\n",
            "Context: ('last', 'six') -> Next Word Counts: Counter({'months': 3})\n",
            "Context: (\"hogan's\", 'patience') -> Next Word Counts: Counter({'ran': 1})\n",
            "Context: ('vaughn', 'knows') -> Next Word Counts: Counter({'better': 1})\n",
            "Context: ('offering', 'tuesday') -> Next Word Counts: Counter({'consisted': 1})\n",
            "Context: ('approach', 'amply') -> Next Word Counts: Counter({'confirm': 1})\n"
          ]
        }
      ],
      "source": [
        "# Create a dictionary of trigram frequencies\n",
        "d = defaultdict(Counter) # defaultdict with Counter as default factory. This allows us to create a nested dictionary where each key maps to another dictionary that counts occurrences.\n",
        "\n",
        "for ngram in freq_tri:\n",
        "    if None not in ngram:\n",
        "        d[ngram[:-1]][ngram[-1]] += freq_tri[ngram]\n",
        "        # ngram[:-1] → All words except the last one → the context (first two words)\n",
        "        # ngram[-1] → The last word → the predicted next word\n",
        "        # freq_tri[ngram] → Frequency count of the trigram i.e. How often that full trigram appeared\n",
        "\n",
        "# Print sample entries from the trigram frequency dictionary\n",
        "print(\"\\nSample Trigram Frequency Dictionary Entries:\")\n",
        "sample_keys = random.sample(list(d.keys()), 5) # Randomly selects 5 contexts (first two words).\n",
        "for key in sample_keys:\n",
        "    print(f\"Context: {key} -> Next Word Counts: {d[key]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5tcWjGz8fd4l",
      "metadata": {
        "id": "5tcWjGz8fd4l"
      },
      "source": [
        "### **Step 7: Define the Text Generation Function**\n",
        "\n",
        "*   Define the **generate_text** function to generate text based on the trigram frequencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "45E4k0DEfnZM",
      "metadata": {
        "id": "45E4k0DEfnZM"
      },
      "outputs": [],
      "source": [
        "# Function to generate text\n",
        "def generate_text(prefix, n=20): # prefix: the starting two words (tuple) — acts as the initial context, Example: (\"machine\", \"learning\"), n: how many words to generate (default = 20)\n",
        "    for _ in range(n):\n",
        "        suffix_candidates = list(d.get(prefix, Counter()).elements())\n",
        "        if not suffix_candidates:\n",
        "            new_prefix = random.choice(unigram), random.choice(unigram)\n",
        "            yield new_prefix[0]  # Yield the first word of the new prefix\n",
        "            prefix = new_prefix\n",
        "        else:\n",
        "            suffix = random.choice(suffix_candidates)\n",
        "            yield suffix\n",
        "            prefix = (*prefix[1:], suffix)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vdBJRj6AftiC",
      "metadata": {
        "id": "vdBJRj6AftiC"
      },
      "source": [
        "### **Step 8: Execute the Text Generation Function**\n",
        "\n",
        "*   Call the **generate_text** function and print the generated text.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2OXd5-fVgDJX",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2OXd5-fVgDJX",
        "outputId": "d5139744-49d3-4731-a96e-07a6148da778"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "road that him her to could it's five of properly `` this the can't always simple on be of declarative\n"
          ]
        }
      ],
      "source": [
        "# Generate text\n",
        "prefix = (\"he\", \"said\")\n",
        "generated_text = list(generate_text(prefix))\n",
        "if generated_text:\n",
        "    print(\" \".join(generated_text))\n",
        "else:\n",
        "    print(\"No text generated.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03c44b0f",
      "metadata": {
        "id": "03c44b0f",
        "outputId": "d619bef5-f3fd-4bd2-ef2d-4f526c07b523",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "two social a was adored if this return could of chow integrated everybody of sinai any now criticism suffering well\n"
          ]
        }
      ],
      "source": [
        "# One more example\n",
        "# Generate text\n",
        "prefix = (\"the\", \"market\")\n",
        "generated_text = list(generate_text(prefix))\n",
        "if generated_text:\n",
        "    print(\" \".join(generated_text))\n",
        "else:\n",
        "    print(\"No text generated.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jozwTLaJUxPo",
      "metadata": {
        "id": "jozwTLaJUxPo"
      },
      "source": [
        "## **Conclusion**\n",
        "\n",
        "This demo showcases NLTK and the Brown corpus for trigram-based Markov chain text generation. Users can run it multiple times to observe the varying generated outputs."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}