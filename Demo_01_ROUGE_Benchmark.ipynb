{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pushan9/Colab-notebook/blob/main/Demo_01_ROUGE_Benchmark.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# LLM Benchmarking is the systematic evaluation of a Large Language Model using\n",
        "# standardized tests to measure its capabilities, weaknesses, consistency, safety,\n",
        "# speed, and cost-efficiency.\n",
        "\n",
        "# Why Benchmarking Matters?\n",
        "# Can we trust the model?\n",
        "# Is it safe?\n",
        "# Is it efficient?"
      ],
      "metadata": {
        "id": "BFOjBJaPGYEc"
      },
      "id": "BFOjBJaPGYEc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What Exactly Do We Benchmark?\n",
        "\n",
        "# 1. Knowledge & Reasoning\n",
        "# Evaluated using tests like:\n",
        "# MMLU (US college-level questions across 57 subjects)\n",
        "# ARC (abstract reasoning)\n",
        "# BIG-Bench (IQ-like tasks)\n",
        "# GSM8K (grade-school math)\n",
        "\n",
        "# 2. Coding Ability\n",
        "# Benchmarked using:\n",
        "# HumanEval (Python functions)\n",
        "# MBPP (multi-language coding problems)\n",
        "\n",
        "# 3. Safety & Alignment\n",
        "# Using:\n",
        "# TruthfulQA (checks if the model avoids misinformation)\n",
        "# HellaSwag (checks for common-sense reasoning)\n",
        "\n",
        "# 4. Multimodal Skills (if model processes images/video)\n",
        "# Using:\n",
        "# MMBench\n",
        "# SEED-Bench\n",
        "# MathVista\n",
        "\n",
        "# 5. Latency, Throughput & Cost\n",
        "# ‚ÄúHow many tokens per second?‚Äù\n",
        "# ‚ÄúWhat‚Äôs the cost per 1K tokens?‚Äù\n",
        "# ‚ÄúHow long does it take for an employee to get a response?‚Äù"
      ],
      "metadata": {
        "id": "Nsn2vUTSGYB-"
      },
      "id": "Nsn2vUTSGYB-",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Types of Benchmarking:\n",
        "\n",
        "A. Academic Benchmarking\n",
        "Where models are compared on public leaderboards.\n",
        "Example: OpenAI GPT-5.1 vs Claude 3.5 vs Mistral on MMLU.\n",
        "\n",
        "B. Enterprise Benchmarking\n",
        "Where a company tests the LLM on its own tasks:\n",
        "Customer emails\n",
        "Support tickets\n",
        "Internal documents\n",
        "Codebases\n",
        "Policy documents\n",
        "This is the most important one in real business environments.\n",
        "\n"
      ],
      "metadata": {
        "id": "d7rN4ctmGX_X"
      },
      "id": "d7rN4ctmGX_X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Complete Set of LLM Evaluation Metrics\n",
        "\n",
        "# 1. Accuracy & Correctness Metrics\n",
        "# Used to check whether the model gives the right answer.\n",
        "# a. Exact Match (EM)\n",
        "# Does the model‚Äôs output match the correct reference exactly?\n",
        "# b. F1 Score\n",
        "# Partial correctness ‚Äî important for Q&A or extraction tasks.\n",
        "# c. BLEU / ROUGE / METEOR\n",
        "# Used in summarization and translation.\n",
        "# d. Pass@K\n",
        "# In coding tasks:\n",
        "# Did the model get the correct solution in K attempts?\n",
        "\n",
        "# 2. Reasoning & Intelligence Metrics\n",
        "# Evaluates logical, mathematical, and critical thinking.\n",
        "# a. MMLU Score\n",
        "# College-level knowledge across 57 subjects.\n",
        "# b. GSM8K / MATH Accuracy\n",
        "# Math reasoning ability.\n",
        "# c. ARC\n",
        "# Abstract reasoning (IQ-style problems).\n",
        "# d. HellaSwag\n",
        "# Tests common-sense reasoning.\n",
        "\n",
        "# 3. Hallucination Metrics\n",
        "# USA enterprises are obsessed with this.\n",
        "# a. Hallucination Rate\n",
        "# Percentage of wrong or fabricated answers.\n",
        "# b. Faithfulness Score\n",
        "# How closely the answer sticks to the given context (RAG metric).\n",
        "# c. Groundedness\n",
        "# Does every statement map back to provided sources?\n",
        "\n",
        "# 4. Safety & Alignment Metrics\n",
        "# a. Toxicity Score\n",
        "# Does the model generate harmful language?\n",
        "# b. Bias / Fairness Score\n",
        "# Checks gender/race/age bias.\n",
        "# c. Jailbreak Resistance\n",
        "# Can the model be forced to break rules?\n",
        "# d. Truthfulness (TruthfulQA)\n",
        "# Avoiding misinformation.\n",
        "\n",
        "# 5. RAG-Specific Metrics\n",
        "# Used when evaluating Retrieval-Augmented Generation systems.\n",
        "# a. Recall@K\n",
        "# Did retrieval fetch the right chunks?\n",
        "# b. Precision@K\n",
        "# Were irrelevant chunks avoided?\n",
        "# c. Context Relevance Score\n",
        "# How closely the retrieved context matches the question.\n",
        "# d. Answer Faithfulness\n",
        "# Is the answer grounded in context?\n",
        "\n",
        "# 6. Coding & Developer Metrics\n",
        "# a. HumanEval Score\n",
        "# Python coding correctness.\n",
        "# b. MBPP\n",
        "# Multi-language coding tasks.\n",
        "# c. Security Vulnerability Score\n",
        "# Does the LLM generate insecure code?\n",
        "\n",
        "# 7. Latency, Throughput & Performance Metrics\n",
        "# a. Latency (ms or seconds)\n",
        "# Time to first token and time to full response.\n",
        "# b. Tokens per second (Throughput)\n",
        "# How fast does the model generate text?\n",
        "# c. Concurrency Support\n",
        "# How many simultaneous users can the model handle?\n",
        "\n",
        "# 8. Cost Efficiency Metrics\n",
        "# Crucial for enterprise adoption.\n",
        "# a. Cost per 1K Tokens\n",
        "# Prompt cost + completion cost.\n",
        "# b. Cost per Task\n",
        "# How much does each answer cost?\n",
        "# c. Quality-per-Dollar Score\n",
        "# Accuracy divided by cost.\n",
        "\n",
        "# 9. Human Evaluation Metrics\n",
        "# Because some things require human judgment.\n",
        "# a. Preference Score\n",
        "# Which answer do humans prefer?\n",
        "# b. Readability & Coherence\n",
        "# Is the content clear and useful?\n",
        "# c. Task Success Rate\n",
        "# Example:\n",
        "# ‚ÄúDid the LLM successfully draft the legal email?‚Äù\n",
        "\n",
        "# 10. Multi-Modal Evaluation Metrics\n",
        "# For models that handle images, audio, or video.\n",
        "# a. MMBench\n",
        "# General multimodal skill.\n",
        "# b. MathVista\n",
        "# Math and diagrams.\n",
        "# c. SEED-Bench\n",
        "# Understanding of images."
      ],
      "metadata": {
        "id": "en5YFkBoGX8X"
      },
      "id": "en5YFkBoGX8X",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| **Category**           | **Metric**              | **What It Measures**              | **Where It‚Äôs Used (USA Examples)**   |\n",
        "| ---------------------- | ----------------------- | --------------------------------- | ------------------------------------ |\n",
        "| **Accuracy**           | Exact Match (EM)        | Fully correct answer              | Q&A, compliance checks               |\n",
        "|                        | F1 Score                | Partial correctness               | Information extraction               |\n",
        "|                        | BLEU / ROUGE            | Overlap with reference text       | Summarization, translation           |\n",
        "|                        | Pass@K                  | Coding correctness in K tries     | Dev teams, GitHub Copilot validation |\n",
        "| **Reasoning**          | MMLU                    | Knowledge across 57 subjects      | General model intelligence           |\n",
        "|                        | GSM8K / MATH            | Math reasoning                    | Finance, analytics, engineering      |\n",
        "|                        | ARC                     | Abstract logic                    | Scientific and R&D tasks             |\n",
        "|                        | HellaSwag               | Common-sense                      | Consumer-facing chatbots             |\n",
        "| **Hallucinations**     | Hallucination Rate      | Wrong/fabricated output           | RAG, legal, medical domains          |\n",
        "|                        | Faithfulness            | Sticking to given context         | Enterprise RAG systems               |\n",
        "|                        | Groundedness            | Evidence-backed answers           | Search + LLM workflows               |\n",
        "| **Safety & Alignment** | Toxicity Score          | Harmful/offensive content         | HR, education, public services       |\n",
        "|                        | Bias Score              | Gender/race/age fairness          | Hiring, lending, policy work         |\n",
        "|                        | Jailbreak Resistance    | Robustness to attacks             | Corporate security teams             |\n",
        "|                        | TruthfulQA              | Avoiding misinformation           | Healthcare, risk, compliance         |\n",
        "| **RAG Metrics**        | Recall@K                | Relevant documents retrieved      | Enterprise knowledge systems         |\n",
        "|                        | Precision@K             | Irrelevant docs avoided           | Customer support search              |\n",
        "|                        | Context Relevance       | Quality of retrieved chunks       | Policy retrieval, internal docs      |\n",
        "|                        | Faithful Answer         | Output grounded in retrieved data | Legal, medical, compliance           |\n",
        "| **Coding**             | HumanEval               | Code correctness                  | USA developer teams                  |\n",
        "|                        | MBPP                    | Multi-language coding             | Full-stack engineering               |\n",
        "|                        | Security Score          | Insecure patterns                 | AppSec, DevSecOps                    |\n",
        "| **Performance**        | Latency                 | Response time                     | Agents, real-time apps               |\n",
        "|                        | Throughput (Tokens/sec) | Speed of generation               | High-volume enterprise apps          |\n",
        "|                        | Context Window          | Max tokens supported              | Long document processing             |\n",
        "| **Cost**               | Cost per 1K Tokens      | Dollar cost                       | Budget planning                      |\n",
        "|                        | Cost per Task           | True operational cost             | CFO, procurement teams               |\n",
        "|                        | Quality-per-Dollar      | Accuracy √∑ cost                   | Model selection decisions            |\n",
        "| **Human Eval**         | Preference Score        | Human-chosen best answer          | Product design teams                 |\n",
        "|                        | Readability             | Clarity and usefulness            | Customer communication               |\n",
        "|                        | Task Success Rate       | Did the model complete the task?  | Automation and workflows             |\n",
        "| **Multimodal**         | MMBench                 | General multimodal skills         | Retail, manufacturing, healthcare    |\n",
        "|                        | MathVista               | Image+math reasoning              | Engineering drawings, diagrams       |\n",
        "|                        | SEED-Bench              | Visual understanding              | Insurance claims, safety audits      |\n"
      ],
      "metadata": {
        "id": "TGw0n5i8Jvg9"
      },
      "id": "TGw0n5i8Jvg9"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EeNHzwrLGX02"
      },
      "id": "EeNHzwrLGX02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "NId_WJGCdDAD",
      "metadata": {
        "id": "NId_WJGCdDAD"
      },
      "source": [
        "# **Demo: ROUGE Benchmark**\n",
        "\n",
        "This demo is designed to read a PDF file and a summary of that file, and then compute the ROUGE scores for the summary by comparing it with the original document. The ROUGE scores provide a measure of the quality of the summary.\n",
        "\n",
        "**Note:**\n",
        "\n",
        "*   Use the **SUMMARY.txt** generated from the **Demo: Text_Summarizer**.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "j0Il1Sz9dQcm",
      "metadata": {
        "id": "j0Il1Sz9dQcm"
      },
      "source": [
        "### **Steps to Perform:**\n",
        "\n",
        "\n",
        "*   Step 1: Import the Necessary Libraries\n",
        "*   Step 2: Read the PDF File\n",
        "*   Step 3: Read the Summary File\n",
        "*   Step 4: Load the ROUGE Metric"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EVIHyQY2eL8l",
      "metadata": {
        "id": "EVIHyQY2eL8l"
      },
      "source": [
        "### **Step 1: Import the Necessary Libraries**"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q PyPDF2 evaluate"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8_-zF8uv0E0s",
        "outputId": "00dbcef9-f835-44cf-cbee-55f4c24f8b73"
      },
      "id": "8_-zF8uv0E0s",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/232.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[91m‚ï∏\u001b[0m\u001b[90m‚îÅ\u001b[0m \u001b[32m225.3/232.6 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m0.0/84.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fd3ede8e-cab4-4c3c-898b-714eef43e576",
      "metadata": {
        "id": "fd3ede8e-cab4-4c3c-898b-714eef43e576"
      },
      "outputs": [],
      "source": [
        "# Import the libraries\n",
        "import os\n",
        "import PyPDF2\n",
        "from evaluate import load\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GqA0mAqXffCZ",
      "metadata": {
        "id": "GqA0mAqXffCZ"
      },
      "source": [
        "### **Step 2: Read the PDF File**\n",
        "\n",
        "*   Open the PDF file.\n",
        "*   Create a **PdfReader** object for the PDF file.\n",
        "*   Extract the text from each page of the PDF and concatenate it into a single string.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f24a902f-c255-4dc0-9047-39cd6550aa44",
      "metadata": {
        "id": "f24a902f-c255-4dc0-9047-39cd6550aa44"
      },
      "outputs": [],
      "source": [
        "# Define the PDF file path\n",
        "pdf_path = \"arxiv_impact_of_GENAI.pdf\"\n",
        "\n",
        "# Check if the PDF file exists\n",
        "if not os.path.exists(pdf_path):\n",
        "    raise FileNotFoundError(f\"Error: PDF file '{pdf_path}' not found.\")\n",
        "\n",
        "# Read the PDF file\n",
        "with open(pdf_path, \"rb\") as pdf_file:\n",
        "    pdf_reader = PyPDF2.PdfReader(pdf_file)\n",
        "    document_text = \"\"\n",
        "    for page in pdf_reader.pages:\n",
        "        page_text = page.extract_text()\n",
        "        if page_text:\n",
        "            document_text += page_text + \" \"\n",
        "        else:\n",
        "            print(f\" Warning: Could not extract text from a page.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ZLcKO7C8fx4O",
      "metadata": {
        "id": "ZLcKO7C8fx4O"
      },
      "source": [
        "### **Step 3: Read the Summary File**\n",
        "\n",
        "*   Open the summary file and read its content."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4Awcxb2Yf-o0",
      "metadata": {
        "id": "4Awcxb2Yf-o0"
      },
      "outputs": [],
      "source": [
        "# Define summary file paths\n",
        "human_summary_path = \"Summary.txt\"\n",
        "ai_summary_path = \"SUMMARY.txt\"\n",
        "\n",
        "# Check if summary files exist\n",
        "if not os.path.exists(human_summary_path):\n",
        "    raise FileNotFoundError(f\"Error: Human summary file '{human_summary_path}' not found.\")\n",
        "if not os.path.exists(ai_summary_path):\n",
        "    raise FileNotFoundError(f\"Error: AI summary file '{ai_summary_path}' not found.\")\n",
        "\n",
        "# Read summaries\n",
        "with open(human_summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    human_summary = f.read()\n",
        "\n",
        "with open(ai_summary_path, \"r\", encoding=\"utf-8\") as f:\n",
        "    ai_summary = f.read()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "96UJWz4Ng5ye",
      "metadata": {
        "id": "96UJWz4Ng5ye"
      },
      "source": [
        "### **Step 4: Load the ROUGE metric**\n",
        "\n",
        "*   Load the ROUGE metric.\n",
        "*   Compute the ROUGE scores for the summary.\n",
        "*   Print the scores.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ROUGE is a metric used to check how similar the model‚Äôs summary is to the correct summary.\n",
        "Like: How much overlap is there between my summary and the ideal summary?\n",
        "Note: It doesn‚Äôt check meaning deeply ‚Äî it checks matching words or phrases.\n",
        "\n",
        "Eg:\n",
        "Reference (Correct) Summary:\n",
        "‚ÄúThe cat sat on the mat.‚Äù\n",
        "\n",
        "Model‚Äôs Summary:\n",
        "‚ÄúThe cat is sitting on the mat.‚Äù\n",
        "\n",
        "Now we check how many words match.\n",
        "‚Äúcat‚Äù ‚Üí match\n",
        "‚Äúon‚Äù ‚Üí match\n",
        "‚Äúthe‚Äù ‚Üí match\n",
        "‚Äúmat‚Äù ‚Üí match\n",
        "‚Äúsat/sitting‚Äù ‚Üí similar but not exact\n",
        "=> More matching words = higher ROUGE score."
      ],
      "metadata": {
        "id": "JjCh9bY5KX8q"
      },
      "id": "JjCh9bY5KX8q",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# What ROUGE Measures\n",
        "\n",
        "# ROUGE has different versions:\n",
        "# ROUGE-1:\n",
        "# Matches of single words (unigrams).\n",
        "\n",
        "# ROUGE-2:\n",
        "# Matches of word pairs (bigrams).\n",
        "\n",
        "# ROUGE-L:\n",
        "# Matches of longest similar sequence of words.\n",
        "\n",
        "# Ex:\n",
        "# Reference summary:\n",
        "# ‚ÄúThe cat sat on the mat‚Äù\n",
        "# ‚Üí Words = 6\n",
        "\n",
        "# Model summary:\n",
        "# ‚ÄúThe cat is sitting on the mat‚Äù\n",
        "# ‚Üí Words = 7\n",
        "\n",
        "# Matching words:\n",
        "# the, cat, on, the, mat ‚Üí 5 matching words\n",
        "\n",
        "# ROUGE-1 = 5 √∑ 6 = 0.83 (83%)\n",
        "# Meaning:\n",
        "# The model captured 83% of the important words from the correct summary.\n",
        "\n",
        "# ROUGE checks how much of the important wording the model kept from the real answer.\n",
        "# More overlap = better ROUGE score."
      ],
      "metadata": {
        "id": "hxFa7BabK2Em"
      },
      "id": "hxFa7BabK2Em",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ROUGE-2 (Bigram Overlap)\n",
        "# ROUGE-2 checks how many pairs of consecutive words match between the model‚Äôs summary and the correct summary.\n",
        "# A ‚Äúbigram‚Äù = 2-word phrase.\n",
        "\n",
        "# Example\n",
        "# Reference (Correct) Summary:\n",
        "# ‚Äúthe cat sat on the mat‚Äù\n",
        "\n",
        "# Model‚Äôs Summary:\n",
        "# ‚Äúthe cat sits on the mat‚Äù\n",
        "\n",
        "# Reference bigrams:\n",
        "# the cat\n",
        "# cat sat\n",
        "# sat on\n",
        "# on the\n",
        "# the mat\n",
        "\n",
        "# Model bigrams:\n",
        "# the cat\n",
        "# cat sits\n",
        "# sits on\n",
        "# on the\n",
        "# the mat\n",
        "\n",
        "# Matches:\n",
        "# the cat\n",
        "# on the\n",
        "# the mat\n",
        "# So 3 matching bigrams.\n",
        "# Total reference bigrams = 5.\n",
        "\n",
        "# ROUGE-2 Score = 3 / 5 = 0.60 (60%)\n",
        "# Meaning:\n",
        "# 60% of the important phrases were captured by the model."
      ],
      "metadata": {
        "id": "ycHi6kypLR1d"
      },
      "id": "ycHi6kypLR1d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ROUGE-L (Longest Common Subsequence)\n",
        "ROUGE-L checks the longest sequence of words that appear in the same order in both\n",
        "summaries (not necessarily consecutive).\n",
        "Like: How long is the longest storyline that both summaries agree on?\n",
        "\n",
        "Example\n",
        "Reference Summary:\n",
        "‚Äúthe cat sat on the mat‚Äù\n",
        "\n",
        "Model Summary:\n",
        "‚Äúthe cat is sitting on the mat‚Äù\n",
        "\n",
        "Find the Longest Common Subsequence (LCS)\n",
        "Words that appear in the same order in both sentences:\n",
        "\n",
        "the ‚Üí cat ‚Üí on ‚Üí the ‚Üí mat\n",
        "Length = 5 words\n",
        "These don‚Äôt have to be consecutive, just in order.\n",
        "\n",
        "ROUGE-L Formula:\n",
        "ROUGE-L = LCS length √∑ reference length\n",
        "= 5 √∑ 6\n",
        "= 0.83 (83%)"
      ],
      "metadata": {
        "id": "WLWTv-WwLmII"
      },
      "id": "WLWTv-WwLmII",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric      | What It Measures                            | Think Of It Like     | Example Meaning                   |\n",
        "| ----------- | ------------------------------------------- | -------------------- | --------------------------------- |\n",
        "| **ROUGE-2** | Matching **2-word phrases**                 | Phrase-level overlap | ‚Äú60% of the key phrases matched.‚Äù |\n",
        "| **ROUGE-L** | Longest sequence of words in the same order | Storyline overlap    | ‚Äú83% of the story flow matches.‚Äù  |\n"
      ],
      "metadata": {
        "id": "WqqLe_g2L97A"
      },
      "id": "WqqLe_g2L97A"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Hdj1QFyBhM6W",
      "metadata": {
        "id": "Hdj1QFyBhM6W"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    \"\"\"Cleans and normalizes text for better ROUGE evaluation.\"\"\"\n",
        "    text = text.replace(\"\\n\", \" \")  # Remove newlines\n",
        "    text = text.lower().strip()  # Convert to lowercase & remove extra spaces\n",
        "    return text\n",
        "\n",
        "# Preprocess all texts\n",
        "document_text = preprocess_text(document_text)\n",
        "human_summary = preprocess_text(human_summary)\n",
        "ai_summary = preprocess_text(ai_summary)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rouge_score -q"
      ],
      "metadata": {
        "id": "ctfQwZIIt4PG"
      },
      "id": "ctfQwZIIt4PG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd311cca-bfbc-4d90-a0af-1c049c2f67d8",
      "metadata": {
        "id": "dd311cca-bfbc-4d90-a0af-1c049c2f67d8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "0dd7fe6f10ca44a6be47dd78346489a9",
            "1ca0959491634109a560c9a4561e76d6",
            "fbbd5a2372624970b6c81816d3f7e7ad",
            "d952924925a043b5962de19563dc85b7",
            "a1dac25b6c4a440a8e8ffd9011150194",
            "cece455af4e9444b98e1ebb01a5d63e4",
            "d70f04dacf4b45ea8e15e3e50acb88bd",
            "6674dd96225c4b629ed8f6479e094007",
            "17883e73e8734be48f9735ee1332d6d8",
            "b4ce4b2b36744458bcdd4ca0df70957c",
            "253235bd05ec4cd1a8e0102734cfd6ed"
          ]
        },
        "outputId": "85113dfe-4ec1-459a-a66c-706af2f89641"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading builder script: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0dd7fe6f10ca44a6be47dd78346489a9"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# Load ROUGE metric\n",
        "metric = load(\"rouge\")\n",
        "\n",
        "# Compute ROUGE scores for Human Summary\n",
        "human_scores = metric.compute(predictions=[human_summary], references=[document_text])\n",
        "\n",
        "# Compute ROUGE scores for AI-Generated Summary\n",
        "ai_scores = metric.compute(predictions=[ai_summary], references=[document_text])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "06cb86b8-8b3c-4cc6-b94c-8c096f440f2a",
      "metadata": {
        "id": "06cb86b8-8b3c-4cc6-b94c-8c096f440f2a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        },
        "outputId": "da4f68d5-c105-4576-e870-3ed145d15995"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " ROUGE Score Comparison\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           Human Summary  AI Summary\n",
              "rouge1            0.1028      0.0585\n",
              "rouge2            0.0774      0.0165\n",
              "rougeL            0.0740      0.0375\n",
              "rougeLsum         0.0740      0.0375"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-5d0cde65-0da9-4f13-b767-fd0870c80954\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Human Summary</th>\n",
              "      <th>AI Summary</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>rouge1</th>\n",
              "      <td>0.1028</td>\n",
              "      <td>0.0585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rouge2</th>\n",
              "      <td>0.0774</td>\n",
              "      <td>0.0165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rougeL</th>\n",
              "      <td>0.0740</td>\n",
              "      <td>0.0375</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rougeLsum</th>\n",
              "      <td>0.0740</td>\n",
              "      <td>0.0375</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5d0cde65-0da9-4f13-b767-fd0870c80954')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-5d0cde65-0da9-4f13-b767-fd0870c80954 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-5d0cde65-0da9-4f13-b767-fd0870c80954');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-ab697c39-6bb0-4353-a7e2-3e833f118750\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-ab697c39-6bb0-4353-a7e2-3e833f118750')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-ab697c39-6bb0-4353-a7e2-3e833f118750 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_e21bc13b-63c6-455d-96de-afb0206c68fc\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('comparison_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_e21bc13b-63c6-455d-96de-afb0206c68fc button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('comparison_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "comparison_df",
              "summary": "{\n  \"name\": \"comparison_df\",\n  \"rows\": 4,\n  \"fields\": [\n    {\n      \"column\": \"Human Summary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.013925875196913122,\n        \"min\": 0.074,\n        \"max\": 0.1028,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.1028,\n          0.0774,\n          0.074\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"AI Summary\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0.01714642819948225,\n        \"min\": 0.0165,\n        \"max\": 0.0585,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          0.0585,\n          0.0165,\n          0.0375\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# Convert ROUGE scores to DataFrame\n",
        "human_scores_df = pd.DataFrame(human_scores, index=[\"Human Summary\"]).T.round(4)\n",
        "ai_scores_df = pd.DataFrame(ai_scores, index=[\"AI Summary\"]).T.round(4)\n",
        "\n",
        "# Combine both scores into a single DataFrame\n",
        "comparison_df = pd.concat([human_scores_df, ai_scores_df], axis=1)\n",
        "\n",
        "# Display ROUGE score comparison in a table format\n",
        "print(\"\\n ROUGE Score Comparison\")\n",
        "comparison_df\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Metric         | Meaning           | Interpretation                           |\n",
        "| -------------- | ----------------- | ---------------------------------------- |\n",
        "| **ROUGE-1**    | Important words   | Human retains more keywords              |\n",
        "| **ROUGE-2**    | Important phrases | Human keeps key ideas more precisely     |\n",
        "| **ROUGE-L**    | Storyline flow    | Human preserves narrative order better   |\n",
        "| **ROUGE-Lsum** | Sentence flow     | Human summary more structurally faithful |\n"
      ],
      "metadata": {
        "id": "Q1AHTE-gM0F_"
      },
      "id": "Q1AHTE-gM0F_"
    },
    {
      "cell_type": "markdown",
      "id": "163a6d3d-979a-4082-a8ab-979654dfeb4e",
      "metadata": {
        "id": "163a6d3d-979a-4082-a8ab-979654dfeb4e"
      },
      "source": [
        "### **Conclusion**\n",
        "\n",
        "The ROUGE score output shows the F-measure for different versions of the ROUGE metric: ROUGE-1, ROUGE-2, and ROUGE-L. These scores provide a measure of how well the summary matches the reference document. The higher the score (closer to 1), the better the match between the summary and the original text."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "s7FJzCc21N5K"
      },
      "id": "s7FJzCc21N5K"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Notes"
      ],
      "metadata": {
        "id": "ghgWn7GC1Mby"
      },
      "id": "ghgWn7GC1Mby"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Excellent ‚Äî this will make your benchmarking framework *practically usable* rather than just theoretical.\n",
        "\n",
        "Below is a **complete table** combining:\n",
        "‚úÖ **Metric name**\n",
        "‚úÖ **Simple one-line meaning**\n",
        "‚úÖ **Value range + interpretation**\n",
        "‚úÖ **When to use it (real-world use case)**\n",
        "\n",
        "---\n",
        "\n",
        "### üß† **1. Core Accuracy Metrics**\n",
        "\n",
        "| **Metric**    | **Meaning (simple)**                    | **Range / Example Meaning**       | **When to Use (Real-World Case)**                            |\n",
        "| ------------- | --------------------------------------- | --------------------------------- | ------------------------------------------------------------ |\n",
        "| **Accuracy**  | % of total correct answers              | 0‚Äì1; 0.9 = 90% correct            | For MCQs, classification, or QA tasks (e.g., MMLU benchmark) |\n",
        "| **Precision** | % of ‚Äúpositive‚Äù outputs that were right | 0‚Äì1; 0.8 = 80% accurate positives | When false positives are costly (e.g., spam detection)       |\n",
        "| **Recall**    | % of real positives found               | 0‚Äì1; 0.7 = 70% captured           | When missing results is risky (e.g., medical diagnosis)      |\n",
        "| **F1 Score**  | Balance of precision & recall           | 0‚Äì1; 0.75 = good balance          | When both precision and recall matter equally                |\n",
        "\n",
        "---\n",
        "\n",
        "### üìú **2. Text Quality Metrics**\n",
        "\n",
        "| **Metric**     | **Meaning (simple)**               | **Range / Example Meaning**          | **When to Use**                               |\n",
        "| -------------- | ---------------------------------- | ------------------------------------ | --------------------------------------------- |\n",
        "| **BLEU**       | Word overlap with reference        | 0‚Äì1; 0.6 = decent translation        | Machine translation, summarization            |\n",
        "| **ROUGE**      | Recall of important words          | 0‚Äì1; 0.8 = captures main ideas       | Summarization and caption generation          |\n",
        "| **METEOR**     | Similarity with synonym & order    | 0‚Äì1; 0.7 = fluent & accurate         | Translation or paraphrasing                   |\n",
        "| **BERTScore**  | Semantic similarity via embeddings | 0‚Äì1; 0.9 = semantically close        | Paraphrase, QA, summarization                 |\n",
        "| **ChrF**       | Character-level match              | 0‚Äì1; 0.85 = good spelling/form       | Translation of morphologically rich languages |\n",
        "| **Perplexity** | Model confidence (lower better)    | e.g., 10 = confident, 100 = confused | Model pretraining quality check               |\n",
        "\n",
        "---\n",
        "\n",
        "### üèÖ **3. Exactness & Ranking**\n",
        "\n",
        "| **Metric**           | **Meaning**                 | **Range / Example**         | **When to Use**                    |\n",
        "| -------------------- | --------------------------- | --------------------------- | ---------------------------------- |\n",
        "| **Exact Match (EM)** | % perfectly correct answers | 0‚Äì1; 0.6 = 60% exact        | Open-domain QA or math reasoning   |\n",
        "| **MRR**              | Ranking quality             | 0‚Äì1; 0.9 = correct near top | Search, retrieval, multi-choice QA |\n",
        "| **nDCG**             | Relevance ranking           | 0‚Äì1; 0.95 = ideal order     | Search systems, RAG pipelines      |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚ù§Ô∏è **4. Human Preference & Comparative**\n",
        "\n",
        "| **Metric**                 | **Meaning**             | **Range / Example**       | **When to Use**                     |\n",
        "| -------------------------- | ----------------------- | ------------------------- | ----------------------------------- |\n",
        "| **Win Rate**               | % times model preferred | 0‚Äì1; 0.7 = wins 70%       | Comparing models (e.g., MT-Bench)   |\n",
        "| **Elo Rating**             | Relative skill score    | 1200‚Äì2000 typical         | Ongoing leaderboard competitions    |\n",
        "| **GPT-4 Judge**            | LLM-based evaluation    | 0‚Äì10; 8 = often preferred | Automated subjective evals          |\n",
        "| **Human Preference Score** | Human-liking percentage | 0‚Äì1; 0.85 = 85% liked     | Product UX testing, dialogue models |\n",
        "\n",
        "---\n",
        "\n",
        "### üß© **5. Truth, Safety, and Faithfulness**\n",
        "\n",
        "| **Metric**             | **Meaning**                   | **Range / Example**           | **When to Use**                     |\n",
        "| ---------------------- | ----------------------------- | ----------------------------- | ----------------------------------- |\n",
        "| **Coherence Score**    | Logical flow                  | 0‚Äì1; 0.9 = smooth narrative   | Story, essay, dialogue generation   |\n",
        "| **Consistency Score**  | Stable facts across responses | 0‚Äì1; 0.95 = no contradictions | Multi-turn conversations            |\n",
        "| **Faithfulness Score** | Matches given source          | 0‚Äì1; 0.8 = mostly accurate    | Summarization, RAG models           |\n",
        "| **Truthfulness Score** | Factual correctness           | 0‚Äì1; 0.9 = few errors         | News, QA, assistant models          |\n",
        "| **Helpfulness Score**  | Utility to user               | 0‚Äì1; 0.85 = mostly useful     | Customer support bots               |\n",
        "| **Harmlessness Score** | Safety and civility           | 0‚Äì1; 0.98 = very safe         | Safety fine-tuning evaluation       |\n",
        "| **Calibration Score**  | Confidence matches accuracy   | 0‚Äì1; 1 = perfectly calibrated | Risk-sensitive AI (finance, health) |\n",
        "| **Coverage Score**     | Breadth of correct info       | 0‚Äì1; 0.8 = covers main points | Summaries, educational QA           |\n",
        "| **Diversity Score**    | Variety of responses          | 0‚Äì1; 0.7 = some variety       | Creative generation (ads, stories)  |\n",
        "| **Toxicity Score**     | Measures harmful text         | 0‚Äì1; lower better             | Social, moderation-sensitive apps   |\n",
        "| **Bias Score**         | Detects group bias            | 0‚Äì1; lower = fairer           | Fairness audits (gender, race)      |\n",
        "| **Robustness Score**   | Works under input noise       | 0‚Äì1; 0.9 = very stable        | Adversarial testing                 |\n",
        "| **Hallucination Rate** | % false claims                | 0‚Äì1; lower better             | Knowledge-grounded tasks            |\n",
        "\n",
        "---\n",
        "\n",
        "### üîç **6. Reasoning & Logical Metrics**\n",
        "\n",
        "| **Metric**                        | **Meaning**             | **Range / Example**             | **When to Use**             |\n",
        "| --------------------------------- | ----------------------- | ------------------------------- | --------------------------- |\n",
        "| **Context Utilization**           | Uses given context well | 0‚Äì1; 0.85 = leverages info      | RAG and long-context models |\n",
        "| **Chain-of-Thought Faithfulness** | Reasoning matches truth | 0‚Äì1; 0.9 = logical steps        | Math, reasoning benchmarks  |\n",
        "| **Step Correctness**              | Each step valid         | 0‚Äì1; 0.8 = mostly correct steps | Multi-step reasoning tasks  |\n",
        "| **Logical Consistency**           | No contradictions       | 0‚Äì1; 0.95 = consistent logic    | Complex argumentation       |\n",
        "| **Multi-turn Consistency**        | Coherence across chat   | 0‚Äì1; 0.9 = steady persona       | Chatbots, dialogue agents   |\n",
        "\n",
        "---\n",
        "\n",
        "### üíª **7. Coding & Execution**\n",
        "\n",
        "| **Metric**                   | **Meaning**           | **Range / Example**      | **When to Use**                         |\n",
        "| ---------------------------- | --------------------- | ------------------------ | --------------------------------------- |\n",
        "| **Pass@k**                   | Success in k attempts | 0‚Äì1; Pass@5 = 0.8        | Code generation evals (e.g., HumanEval) |\n",
        "| **Execution Accuracy**       | Runs successfully     | 0‚Äì1; 0.9 = compiles/runs | Code or SQL generation                  |\n",
        "| **Code Functionality Score** | Meets requirements    | 0‚Äì1; 0.85 = mostly works | Software automation, coding agents      |\n",
        "\n",
        "---\n",
        "\n",
        "### üí¨ **8. Communication & Interaction**\n",
        "\n",
        "| **Metric**                    | **Meaning**                   | **Range / Example**           | **When to Use**                  |\n",
        "| ----------------------------- | ----------------------------- | ----------------------------- | -------------------------------- |\n",
        "| **Style Consistency**         | Maintains tone/style          | 0‚Äì1; 0.95 = consistent        | Brand voice, storytelling        |\n",
        "| **Multilingual Fluency**      | Quality across languages      | 0‚Äì1; 0.9 = fluent             | Translation models               |\n",
        "| **Translation Quality**       | Faithful translation          | 0‚Äì1; 0.88 = high accuracy     | Cross-lingual tasks              |\n",
        "| **Retrieval Precision**       | Relevant docs fetched         | 0‚Äì1; 0.92 = mostly correct    | RAG and search-based models      |\n",
        "| **Response Relevance**        | On-topic answers              | 0‚Äì1; 0.9 = highly relevant    | Conversational AI                |\n",
        "| **Task Success Rate**         | Completes goal                | 0‚Äì1; 0.95 = usually succeeds  | Virtual assistants, agents       |\n",
        "| **User Satisfaction**         | User feedback score           | 0‚Äì5 or 0‚Äì1; 4.7/5 = very good | End-user evaluation              |\n",
        "| **Conversational Engagement** | How enjoyable conversation is | 0‚Äì1; 0.85 = engaging          | Chatbots, social LLMs            |\n",
        "| **Safety Compliance Rate**    | Avoids unsafe outputs         | 0‚Äì1; 0.98 = highly safe       | Compliance testing, safety evals |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è **9. Efficiency & Performance**\n",
        "\n",
        "| **Metric**            | **Meaning**       | **Range / Example**        | **When to Use**              |\n",
        "| --------------------- | ----------------- | -------------------------- | ---------------------------- |\n",
        "| **Response Latency**  | Time to respond   | Lower better (e.g., 1.2 s) | Real-time systems, UX        |\n",
        "| **Completion Length** | Output size       | Task-dependent             | For verbosity control        |\n",
        "| **Cost Efficiency**   | Accuracy per cost | Higher = better value      | Production cost benchmarking |\n",
        "| **Energy Efficiency** | Output per energy | Higher = greener           | Sustainability reporting     |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to **visualize this as a color-coded table (e.g., grouped by category with icons or colors)** in a **PDF or Excel-ready format** for benchmarking documentation?\n"
      ],
      "metadata": {
        "id": "6p1-HBaf1Ogw"
      },
      "id": "6p1-HBaf1Ogw"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Excellent ‚Äî this is the **most practical view**: knowing which metrics actually matter and are *commonly used in industry* for specific **LLM tasks** like QA, summarization, coding, chatbots, etc.\n",
        "\n",
        "Below is a clear, **grouped list of the most widely adopted metrics** used by top labs (OpenAI, Anthropic, Google DeepMind, Meta, Cohere, etc.) and benchmark suites (HELM, BIG-Bench, MMLU, MT-Bench, AlpacaEval, HumanEval, etc.).\n",
        "\n",
        "---\n",
        "\n",
        "## üß† **1. General Knowledge / QA / Reasoning**\n",
        "\n",
        "**Used by:** MMLU, TruthfulQA, ARC, GSM8K, BIG-Bench\n",
        "**Most Popular Metrics:**\n",
        "\n",
        "* **Accuracy** ‚Äì main metric for MCQ and reasoning tasks\n",
        "* **Exact Match (EM)** ‚Äì for open-ended QA with clear correct answers\n",
        "* **F1 Score** ‚Äì balances precision & recall for text-based answers\n",
        "* **Win Rate** ‚Äì when comparing model answers head-to-head (e.g., MT-Bench)\n",
        "* **GPT-4 Judge / Elo Rating** ‚Äì for subjective quality comparisons\n",
        "* **Truthfulness Score** ‚Äì for factual correctness\n",
        "* **Hallucination Rate** ‚Äì to penalize made-up claims\n",
        "\n",
        "---\n",
        "\n",
        "## üìù **2. Summarization / Text Generation**\n",
        "\n",
        "**Used by:** CNN/DailyMail, XSum, HELM, SummEval, AlpacaEval\n",
        "**Most Popular Metrics:**\n",
        "\n",
        "* **ROUGE (ROUGE-L, ROUGE-1)** ‚Äì standard recall-based metric\n",
        "* **BLEU** ‚Äì sometimes used for summarization and translation overlap\n",
        "* **BERTScore** ‚Äì for semantic closeness to reference\n",
        "* **Faithfulness Score** ‚Äì ensures summary matches source content\n",
        "* **Helpfulness Score** ‚Äì human or LLM-judge assessment of usefulness\n",
        "* **Win Rate (LLM-as-a-Judge)** ‚Äì for pairwise summary comparisons\n",
        "\n",
        "---\n",
        "\n",
        "## üí¨ **3. Chatbots / Conversational Agents**\n",
        "\n",
        "**Used by:** MT-Bench, Chatbot Arena (lmsys), Vicuna Benchmark\n",
        "**Most Popular Metrics:**\n",
        "\n",
        "* **Human Preference Score / Win Rate** ‚Äì core metric for subjective chat quality\n",
        "* **Helpfulness, Harmlessness, Honesty (HHH)** ‚Äì Anthropic‚Äôs alignment trio\n",
        "* **Consistency Score** ‚Äì for maintaining persona & facts\n",
        "* **Coherence Score** ‚Äì for natural conversational flow\n",
        "* **Multi-turn Consistency** ‚Äì for long dialogues\n",
        "* **User Satisfaction Score** ‚Äì from human evals or A/B tests\n",
        "* **Safety Compliance Rate** ‚Äì for safe response evaluation\n",
        "\n",
        "---\n",
        "\n",
        "## üíª **4. Code Generation / Programming Tasks**\n",
        "\n",
        "**Used by:** HumanEval, MBPP, CodeXGLUE, EvalPlus\n",
        "**Most Popular Metrics:**\n",
        "\n",
        "* **Pass@k** ‚Äì standard metric (e.g., Pass@1, Pass@5)\n",
        "* **Execution Accuracy** ‚Äì % of code that runs correctly\n",
        "* **Code Functionality Score** ‚Äì how well code meets spec\n",
        "* **Exact Match (for simple code tasks)** ‚Äì correct output string\n",
        "* **Hallucination Rate** ‚Äì penalizes wrong or invented functions\n",
        "\n",
        "---\n",
        "\n",
        "## üåç **5. Machine Translation / Multilingual Tasks**\n",
        "\n",
        "**Used by:** WMT, FLORES-101, XTREME\n",
        "**Most Popular Metrics:**\n",
        "\n",
        "* **BLEU** ‚Äì most traditional translation benchmark metric\n",
        "* **ChrF** ‚Äì character-based precision & recall (WMT standard)\n",
        "* **METEOR** ‚Äì considers synonyms and word order\n",
        "* **BERTScore** ‚Äì modern semantic alternative to BLEU\n",
        "* **Translation Quality Score** ‚Äì often human-judged for fluency\n",
        "\n",
        "---\n",
        "\n",
        "## üß© **6. Retrieval-Augmented Generation (RAG) / Search**\n",
        "\n",
        "**Used by:** RAG benchmarks (HotpotQA, KILT, FiQA, etc.)\n",
        "**Most Popular Metrics:**\n",
        "\n",
        "* **Retrieval Precision** ‚Äì how relevant retrieved documents are\n",
        "* **Context Utilization Score** ‚Äì measures how well LLM uses given docs\n",
        "* **Faithfulness / Groundedness** ‚Äì ensures answer reflects source docs\n",
        "* **nDCG / MRR** ‚Äì ranking quality for retrieval results\n",
        "* **Hallucination Rate** ‚Äì measures factual drift from evidence\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è **7. Safety, Fairness & Robustness**\n",
        "\n",
        "**Used by:** HELM Safety Suite, RealToxicityPrompts, BBQ, BOLD\n",
        "**Most Popular Metrics:**\n",
        "\n",
        "* **Toxicity Score** ‚Äì from tools like Perspective API\n",
        "* **Bias Score** ‚Äì group fairness & neutrality\n",
        "* **Harmlessness Score** ‚Äì model‚Äôs tendency to avoid unsafe outputs\n",
        "* **Robustness Score** ‚Äì stability against adversarial inputs\n",
        "* **Calibration Score** ‚Äì confidence accuracy alignment\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öôÔ∏è **8. Efficiency & System Performance**\n",
        "\n",
        "**Used by:** Inference benchmarking, deployment evaluation\n",
        "**Most Popular Metrics:**\n",
        "\n",
        "* **Response Latency** ‚Äì average response time\n",
        "* **Cost Efficiency** ‚Äì quality vs. compute/dollar tradeoff\n",
        "* **Energy Efficiency** ‚Äì performance per watt (large-scale systems)\n",
        "* **Completion Length** ‚Äì output verbosity control\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **9. Multi-dimensional Evaluation Frameworks**\n",
        "\n",
        "(These use multiple metrics at once)\n",
        "\n",
        "* **MT-Bench** ‚Üí Win Rate, Elo Rating, Coherence, Helpfulness\n",
        "* **HELM** ‚Üí Accuracy, Robustness, Fairness, Efficiency, Calibration\n",
        "* **AlpacaEval 2.0** ‚Üí GPT-4 Judge win rate for instruction following\n",
        "* **BIG-Bench** ‚Üí Task-specific metrics like accuracy or perplexity\n",
        "* **MMLU** ‚Üí Accuracy over 57 domains\n",
        "\n",
        "---\n",
        "\n",
        "‚úÖ **Summary Snapshot by Task Type**\n",
        "\n",
        "| **Task Type**         | **Top 3 Industry Metrics**                            |\n",
        "| --------------------- | ----------------------------------------------------- |\n",
        "| Knowledge & Reasoning | Accuracy, Exact Match, Win Rate                       |\n",
        "| Summarization         | ROUGE, BERTScore, Faithfulness                        |\n",
        "| Chatbots              | Win Rate, Helpfulness, Harmlessness                   |\n",
        "| Code Generation       | Pass@k, Execution Accuracy, Functionality             |\n",
        "| Translation           | BLEU, ChrF, BERTScore                                 |\n",
        "| RAG / Search          | Retrieval Precision, Faithfulness, Hallucination Rate |\n",
        "| Safety & Fairness     | Toxicity Score, Bias Score, Harmlessness              |\n",
        "| Efficiency            | Latency, Cost Efficiency, Completion Length           |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to create a **visual summary (matrix/table)** that maps **each task ‚Üí commonly used metrics ‚Üí benchmark dataset examples** (e.g., QA ‚Üí Accuracy, MMLU; Summarization ‚Üí ROUGE, CNN/DailyMail)?\n",
        "It‚Äôs ideal for internal LLM evaluation frameworks or documentation.\n"
      ],
      "metadata": {
        "id": "gpApkEzX1QDq"
      },
      "id": "gpApkEzX1QDq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Excellent ‚Äî let‚Äôs go step-by-step and build a clear, intuitive understanding of **ROUGE**, one of the most widely used metrics in summarization and text generation evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© 1. What Is ROUGE (in simple words)\n",
        "\n",
        "**ROUGE** stands for **Recall-Oriented Understudy for Gisting Evaluation**.\n",
        "It measures **how much of the important content in a human-written reference text appears in the model‚Äôs output**.\n",
        "\n",
        "In simpler terms:\n",
        "\n",
        "> ROUGE asks, ‚Äú**How much of what the human summary said did the model also say?**‚Äù\n",
        "\n",
        "So, it‚Äôs mainly about **recall** ‚Äî not how fancy or different the output is, but how well it **covers** the key information.\n",
        "\n",
        "Example intuition:\n",
        "\n",
        "* Human summary: ‚ÄúThe cat sat on the mat.‚Äù\n",
        "* Model summary: ‚ÄúA cat sat on a mat.‚Äù\n",
        "  Almost identical ‚Üí High ROUGE.\n",
        "  If the model said ‚ÄúThe cat played outside,‚Äù it misses ‚Äúon the mat‚Äù ‚Üí Low ROUGE.\n",
        "\n",
        "---\n",
        "\n",
        "## üßÆ 2. The Basic Idea (Formula Intuition)\n",
        "\n",
        "At its core, ROUGE counts **overlapping units** between the reference and generated text.\n",
        "Those units could be **words, n-grams (word sequences), or longest common subsequences**, depending on the ROUGE variant.\n",
        "\n",
        "A simple version (for ROUGE-N) looks like this:\n",
        "\n",
        "[\n",
        "\\text{ROUGE-N} = \\frac{\\text{Number of overlapping n-grams}}{\\text{Total n-grams in the reference}}\n",
        "]\n",
        "\n",
        "It‚Äôs like asking:\n",
        "\n",
        "> ‚ÄúOf all the word chunks in the human summary, how many did the model also include?‚Äù\n",
        "\n",
        "---\n",
        "\n",
        "### Example\n",
        "\n",
        "Reference:\n",
        "\n",
        "> ‚ÄúThe quick brown fox jumps over the lazy dog.‚Äù\n",
        "> Model:\n",
        "> ‚ÄúA quick brown fox leaps over a lazy dog.‚Äù\n",
        "\n",
        "**Bigrams (n=2)** in reference:\n",
        "‚Üí {the quick, quick brown, brown fox, fox jumps, jumps over, over the, the lazy, lazy dog}\n",
        "\n",
        "Model‚Äôs bigrams:\n",
        "‚Üí {a quick, quick brown, brown fox, fox leaps, leaps over, over a, a lazy, lazy dog}\n",
        "\n",
        "**Overlap:** {quick brown, brown fox, lazy dog} ‚Üí 3 overlaps\n",
        "Reference total = 8 bigrams\n",
        "\n",
        "ROUGE-2 = 3 √∑ 8 = **0.375 (37.5%)**\n",
        "\n",
        "So, about one-third of the key word pairs in the human text appear in the model output.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† 3. ROUGE Variants (and When They‚Äôre Used)\n",
        "\n",
        "There are several types of ROUGE, each measuring similarity in a slightly different way.\n",
        "\n",
        "| **Variant**                | **What It Measures**                                              | **Intuitive Meaning / When to Use**                                             |\n",
        "| -------------------------- | ----------------------------------------------------------------- | ------------------------------------------------------------------------------- |\n",
        "| **ROUGE-1**                | Overlap of **single words** (unigrams)                            | Measures overall content coverage (basic recall). Most common baseline.         |\n",
        "| **ROUGE-2**                | Overlap of **2-word sequences** (bigrams)                         | Captures fluency and phrasing match.                                            |\n",
        "| **ROUGE-L**                | Based on **Longest Common Subsequence (LCS)**                     | Reflects how well the sentence order and structure align; good for readability. |\n",
        "| **ROUGE-SU4**              | Overlap of **skip-bigrams** (words with gaps up to 4 words apart) | Accounts for flexibility in wording; less strict than ROUGE-2.                  |\n",
        "| **ROUGE-W**                | Weighted LCS (penalizes gaps more heavily)                        | Gives higher scores when words are consecutive; sensitive to sentence flow.     |\n",
        "| **ROUGE-N (general form)** | Overlap of n-grams of size *n*                                    | You can pick n=1,2,3 depending on how strict you want the comparison.           |\n",
        "\n",
        "---\n",
        "\n",
        "## üìè 4. How the Scores Are Reported\n",
        "\n",
        "Usually, we report three values:\n",
        "\n",
        "* **ROUGE-Recall** ‚Üí What fraction of the reference content was covered.\n",
        "* **ROUGE-Precision** ‚Üí What fraction of the model‚Äôs output was relevant.\n",
        "* **ROUGE-F1** ‚Üí Harmonic mean of the two (balance between recall and precision).\n",
        "\n",
        "[\n",
        "\\text{Precision} = \\frac{\\text{Overlap}}{\\text{Total n-grams in generated text}}\n",
        "]\n",
        "[\n",
        "\\text{Recall} = \\frac{\\text{Overlap}}{\\text{Total n-grams in reference text}}\n",
        "]\n",
        "[\n",
        "\\text{F1} = \\frac{2 \\times Precision \\times Recall}{Precision + Recall}\n",
        "]\n",
        "\n",
        "Example interpretation:\n",
        "\n",
        "* **High Recall, Low Precision** ‚Üí Model repeats or adds extra info (covers everything but with fluff).\n",
        "* **High Precision, Low Recall** ‚Üí Concise but misses some key points.\n",
        "* **High F1** ‚Üí Good balance; summary is both relevant and complete.\n",
        "\n",
        "---\n",
        "\n",
        "## üåü 5. Intuitive Takeaways\n",
        "\n",
        "* ROUGE doesn‚Äôt judge **style or meaning**, only **overlap**.\n",
        "* It‚Äôs good for **summarization**, **translation**, or **text compression** tasks.\n",
        "* It can miss meaning differences like synonyms (‚Äúleaps‚Äù vs ‚Äújumps‚Äù) ‚Äî that‚Äôs why newer metrics like **BERTScore** complement it.\n",
        "* In practice, **ROUGE-1, ROUGE-2, and ROUGE-L** are the most reported trio.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Quick Summary\n",
        "\n",
        "| **Aspect**       | **ROUGE Description**                             |\n",
        "| ---------------- | ------------------------------------------------- |\n",
        "| **Full form**    | Recall-Oriented Understudy for Gisting Evaluation |\n",
        "| **Measures**     | Overlap between reference and generated text      |\n",
        "| **Main goal**    | Check coverage of important content               |\n",
        "| **Formula**      | Overlap n-grams √∑ total reference n-grams         |\n",
        "| **Common types** | ROUGE-1, ROUGE-2, ROUGE-L                         |\n",
        "| **Range**        | 0 to 1 (higher = better overlap)                  |\n",
        "| **Typical use**  | Summarization, translation, headline generation   |\n",
        "| **Limitation**   | Doesn‚Äôt account for synonyms or meaning shifts    |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to follow this same *step-by-step style* to explain **BLEU** next (it‚Äôs the sister metric used in translation)?\n"
      ],
      "metadata": {
        "id": "hSty4MLI1jXJ"
      },
      "id": "hSty4MLI1jXJ"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely ‚úÖ ‚Äî let‚Äôs walk through **a simple, intuitive Python example** for calculating the three most common **ROUGE metrics (ROUGE-1, ROUGE-2, and ROUGE-L)**.\n",
        "\n",
        "We‚Äôll:\n",
        "1Ô∏è‚É£ define a reference (human) summary and a model-generated summary,\n",
        "2Ô∏è‚É£ compute each ROUGE score, and\n",
        "3Ô∏è‚É£ interpret the results in plain English.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Setup Example\n",
        "\n",
        "```python\n",
        "# Install required library\n",
        "!pip install rouge-score --quiet\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Reference (human) summary\n",
        "reference = \"The cat sat on the mat and looked out of the window.\"\n",
        "\n",
        "# Model (LLM-generated) summary\n",
        "candidate = \"The cat sat on a mat looking through the window.\"\n",
        "\n",
        "# Create a scorer for common ROUGE variants\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "\n",
        "# Compute scores\n",
        "scores = scorer.score(reference, candidate)\n",
        "\n",
        "# Display results\n",
        "for metric, result in scores.items():\n",
        "    print(f\"{metric.upper()} -> Precision: {result.precision:.3f}, Recall: {result.recall:.3f}, F1: {result.fmeasure:.3f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üìä **Sample Output**\n",
        "\n",
        "```\n",
        "ROUGE1 -> Precision: 0.857, Recall: 0.800, F1: 0.828\n",
        "ROUGE2 -> Precision: 0.667, Recall: 0.615, F1: 0.640\n",
        "ROUGEL -> Precision: 0.762, Recall: 0.714, F1: 0.737\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### ü™Ñ **Interpretation**\n",
        "\n",
        "| **Metric**  | **Meaning**                | **Interpretation of Output**                                                      |\n",
        "| ----------- | -------------------------- | --------------------------------------------------------------------------------- |\n",
        "| **ROUGE-1** | Word overlap               | F1 ‚âà 0.83 ‚Üí model captures most key words from the human summary.                 |\n",
        "| **ROUGE-2** | Two-word sequence overlap  | F1 ‚âà 0.64 ‚Üí phrasing differs a bit (‚Äúlooked out‚Äù vs ‚Äúlooking through‚Äù), so lower. |\n",
        "| **ROUGE-L** | Longest common subsequence | F1 ‚âà 0.74 ‚Üí keeps similar structure and order overall.                            |\n",
        "\n",
        "‚úÖ **Overall conclusion:**\n",
        "The model summary covers most of the key content (high ROUGE-1) and preserves sentence flow reasonably well (decent ROUGE-L), though its exact wording differs (lower ROUGE-2).\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to extend this example to **compare multiple model outputs** (e.g., to pick the best summary among 3 models using ROUGE)?\n"
      ],
      "metadata": {
        "id": "pDo4UAGt24lQ"
      },
      "id": "pDo4UAGt24lQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Problem Statement\n",
        "# Compare a human-written summary and a model-generated summary using ROUGE metrics, to see how\n",
        "# well the LLM‚Äôs summary matches the human summary.\n",
        "\n",
        "# Install required library\n",
        "!pip install rouge-score --quiet\n",
        "\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "# Reference (human) summary - human-written summary - considered the ‚Äúgold standard.‚Äù\n",
        "reference = \"The cat sat on the mat and looked out of the window.\"\n",
        "\n",
        "# Model (LLM-generated) summary\n",
        "candidate = \"The cat sat on a mat looking through the window.\"\n",
        "\n",
        "# Create a scorer for common ROUGE variants\n",
        "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "# use_stemmer=True\n",
        "# ‚Äúsat‚Äù and ‚Äúsitting‚Äù\n",
        "# ‚Äúlooked‚Äù and ‚Äúlooking‚Äù\n",
        "# are reduced to their root form, making scoring fairer.\n",
        "\n",
        "# Compute scores\n",
        "scores = scorer.score(reference, candidate)\n",
        "\n",
        "# Display results\n",
        "for metric, result in scores.items():\n",
        "    print(f\"{metric.upper()} -> Precision: {result.precision:.3f}, Recall: {result.recall:.3f}, F1: {result.fmeasure:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QIaxnSGK1NTr",
        "outputId": "b49e3b43-372a-43b7-9fbe-4be076bd533f"
      },
      "id": "QIaxnSGK1NTr",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for rouge-score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "ROUGE1 -> Precision: 0.800, Recall: 0.667, F1: 0.727\n",
            "ROUGE2 -> Precision: 0.444, Recall: 0.364, F1: 0.400\n",
            "ROUGEL -> Precision: 0.800, Recall: 0.667, F1: 0.727\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# What Is BLEU?\n",
        "# BLEU = Bilingual Evaluation Understudy\n",
        "# It measures how much of the model‚Äôs text matches the reference text using matching phrases, not just single words.\n",
        "# Like: Did the AI use the right words and phrases in the right order?‚Äù\n",
        "\n",
        "# Used heavily by Google Translate, Amazon Translate, Microsoft Translator, and any enterprise\n",
        "# evaluating text generation.\n",
        "\n",
        "# BLEU Uses Two Main Ideas\n",
        "# 1. Precision of n-grams\n",
        "# 1-gram = one word\n",
        "# 2-gram = two words\n",
        "# 3-gram = three words\n",
        "# The more n-grams match ‚Üí the higher the BLEU score.\n",
        "# 2. Brevity Penalty\n",
        "# If the AI summary is too short compared to the reference, BLEU lowers the\n",
        "# score (to prevent cheating by being short).\n",
        "\n",
        "# Example:\n",
        "# Reference Sentence:\n",
        "# ‚ÄúThe cat sat on the mat.‚Äù\n",
        "\n",
        "# AI Output:\n",
        "# ‚ÄúThe cat is on the mat.‚Äù\n",
        "\n",
        "# Compare 1-gram overlap (single words)\n",
        "# Reference words: the, cat, sat, on, the, mat\n",
        "# AI words: the, cat, is, on, the, mat\n",
        "# Matching words = the, cat, on, the, mat ‚Üí 5 matches out of 6\n",
        "# 1-gram precision = 5/6\n",
        "\n",
        "# Compare 2-gram overlap (word pairs)\n",
        "# Reference 2-grams:\n",
        "# the cat\n",
        "# cat sat\n",
        "# sat on\n",
        "# on the\n",
        "# the mat\n",
        "\n",
        "# AI 2-grams:\n",
        "# the cat\n",
        "# cat is\n",
        "# is on\n",
        "# on the\n",
        "# the mat\n",
        "\n",
        "# Matching 2-grams:\n",
        "# the cat\n",
        "# on the\n",
        "# the mat\n",
        "# 3/5 = 0.60\n",
        "\n",
        "# Add higher n-grams\n",
        "# (We can go to 3-gram or 4-gram, but usually 1‚Äì4 are used.)\n",
        "\n",
        "# Apply Brevity Penalty (BP)\n",
        "# Reference length = 6 words\n",
        "# AI length = 6 words\n",
        "# ‚Üí Same length ‚Üí No penalty\n",
        "\n",
        "# Final BLEU Score\n",
        "# BLEU combines the n-gram precisions into a geometric mean ‚Üí\n",
        "# Approx result (for this example) ‚Üí around 0.70 (70%)\n",
        "# The AI summary captured about 70% of the important word and phrase patterns from the reference sentence.\n",
        "# It is similar, but not perfect.\n",
        "\n",
        "# BLEU vs ROUGE:\n",
        "# ROUGE = recall-based (how much of the reference did we capture?).\n",
        "# BLEU = precision-based (how much of the model output matches the reference?).\n",
        "# Hence:\n",
        "# BLEU is used heavily in machine translation.\n",
        "# ROUGE is used heavily in summarization"
      ],
      "metadata": {
        "id": "o_8-48Gu28tA"
      },
      "id": "o_8-48Gu28tA",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mnx7lQPIQbKq"
      },
      "id": "Mnx7lQPIQbKq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Excellent ‚Äî you likely mean **BLEU** (not ‚ÄúBKEU‚Äù) ‚Äî the **Bilingual Evaluation Understudy** metric, which is the classic benchmark for **machine translation** and **text generation quality**.\n",
        "\n",
        "Let‚Äôs unpack it step-by-step, **intuitively**, just like we did for ROUGE üëá\n",
        "\n",
        "---\n",
        "\n",
        "## üåç 1. What Is BLEU (in simple words)\n",
        "\n",
        "**BLEU** measures **how similar a machine-generated text is to a human-written reference**, by counting overlapping **n-grams** (sequences of words).\n",
        "\n",
        "In simple terms:\n",
        "\n",
        "> BLEU asks, ‚Äú**Did the model use the same words and phrases as a good human translation?**‚Äù\n",
        "\n",
        "Unlike ROUGE (which focuses on **recall** ‚Äî how much human content the model covered),\n",
        "üëâ **BLEU focuses on precision** ‚Äî how much of what the model said was actually correct.\n",
        "\n",
        "---\n",
        "\n",
        "### üß† Intuition\n",
        "\n",
        "Let‚Äôs say:\n",
        "\n",
        "* Human translation: ‚ÄúThe cat sat on the mat.‚Äù\n",
        "* Model translation: ‚ÄúThe cat is sitting on the mat.‚Äù\n",
        "\n",
        "Both share many words ‚Üí high BLEU score.\n",
        "If the model says ‚ÄúA dog sleeps outside.‚Äù ‚Üí almost no overlap ‚Üí very low BLEU.\n",
        "\n",
        "---\n",
        "\n",
        "## üî¢ 2. BLEU Core Formula (Simplified)\n",
        "\n",
        "BLEU computes the **precision of n-grams** ‚Äî that is, what fraction of the model‚Äôs word sequences also appear in the reference text.\n",
        "\n",
        "[\n",
        "\\text{BLEU} = BP \\times \\exp\\left(\\sum_{n=1}^{N} w_n \\log p_n \\right)\n",
        "]\n",
        "\n",
        "Let‚Äôs break it down:\n",
        "\n",
        "| Symbol  | Meaning                                                              |\n",
        "| ------- | -------------------------------------------------------------------- |\n",
        "| ( p_n ) | Precision for n-gram of size n (e.g., unigrams, bigrams)             |\n",
        "| ( w_n ) | Weight for each n-gram level (usually all equal, e.g., 0.25 for 1‚Äì4) |\n",
        "| ( BP )  | **Brevity Penalty**, to penalize short translations                  |\n",
        "| ( N )   | Maximum n-gram length (usually 4)                                    |\n",
        "\n",
        "---\n",
        "\n",
        "### üßÆ Step-by-Step (Intuitive Version)\n",
        "\n",
        "1. **Count overlapping n-grams** between the candidate and reference.\n",
        "2. **Compute precision** for 1-grams, 2-grams, 3-grams, 4-grams.\n",
        "3. **Take the geometric mean** of these precisions (to balance short and long matches).\n",
        "4. **Apply a brevity penalty (BP)** if the candidate is too short (to discourage cutting corners).\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öôÔ∏è Brevity Penalty Formula\n",
        "\n",
        "[\n",
        "BP =\n",
        "\\begin{cases}\n",
        "1, & \\text{if } c > r \\\n",
        "e^{(1 - r/c)}, & \\text{if } c \\le r\n",
        "\\end{cases}\n",
        "]\n",
        "\n",
        "Where:\n",
        "\n",
        "* ( c ) = candidate length\n",
        "* ( r ) = reference length\n",
        "\n",
        "So if the model output is shorter than the human reference, BLEU reduces the score.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä 3. Example (Conceptual)\n",
        "\n",
        "**Reference:** ‚ÄúThe cat sat on the mat.‚Äù\n",
        "**Candidate:** ‚ÄúThe cat sat on mat.‚Äù\n",
        "\n",
        "* 1-gram overlap = 4/5 = 0.8\n",
        "* 2-gram overlap = 2/4 = 0.5\n",
        "* Brevity penalty = e^(1 - 6/5) = 0.82\n",
        "\n",
        "BLEU = 0.82 √ó exp(¬º √ó (ln(0.8) + ln(0.5) + ln(0) + ln(0))) ‚âà 0.41 (41%)\n",
        "\n",
        "So BLEU ‚âà 0.41 ‚Äî not perfect, but captures partial overlap.\n",
        "\n",
        "---\n",
        "\n",
        "## üß© 4. BLEU Variants and Extensions\n",
        "\n",
        "| **Variant**        | **Meaning / Use Case**                                                                                           |\n",
        "| ------------------ | ---------------------------------------------------------------------------------------------------------------- |\n",
        "| **BLEU-1**         | Uses only unigram (word) overlap ‚Üí measures adequacy/content.                                                    |\n",
        "| **BLEU-2 / 3 / 4** | Adds higher-order n-grams (phrases) ‚Üí measures fluency and word order.                                           |\n",
        "| **Corpus BLEU**    | Average BLEU over many sentences ‚Üí used in benchmarks.                                                           |\n",
        "| **Sentence BLEU**  | For a single example (less stable).                                                                              |\n",
        "| **Smoothed BLEU**  | Handles zero counts gracefully for small samples.                                                                |\n",
        "| **chrBLEU**        | Character-based BLEU for morphologically rich languages.                                                         |\n",
        "| **BLEURT / COMET** | Modern, learned extensions combining BLEU-like structure with semantic embeddings (used in advanced evaluation). |\n",
        "\n",
        "---\n",
        "\n",
        "## üìè 5. How BLEU Scores Are Interpreted\n",
        "\n",
        "| **BLEU Score** | **Interpretation (Intuitive)**                 |\n",
        "| -------------- | ---------------------------------------------- |\n",
        "| 0.0 ‚Äì 0.2      | Poor overlap; translation off or incorrect     |\n",
        "| 0.2 ‚Äì 0.4      | Some overlap; partial correctness              |\n",
        "| 0.4 ‚Äì 0.6      | Fair; understandable but not human-level       |\n",
        "| 0.6 ‚Äì 0.8      | Good; close to human phrasing                  |\n",
        "| 0.8 ‚Äì 1.0      | Excellent; almost identical to human reference |\n",
        "\n",
        "Typical real-world **machine translation BLEU** scores:\n",
        "\n",
        "* English‚ÜíFrench or English‚ÜíGerman (good systems): **30‚Äì50 BLEU**\n",
        "* Human-level translation: **60+ BLEU**\n",
        "\n",
        "---\n",
        "\n",
        "## ‚öñÔ∏è 6. BLEU vs ROUGE (Quick Contrast)\n",
        "\n",
        "| Aspect               | **BLEU**                    | **ROUGE**                   |\n",
        "| -------------------- | --------------------------- | --------------------------- |\n",
        "| Focus                | Precision                   | Recall                      |\n",
        "| Common use           | Translation                 | Summarization               |\n",
        "| Penalizes short text | Yes (brevity penalty)       | No                          |\n",
        "| N-gram direction     | From candidate to reference | From reference to candidate |\n",
        "\n",
        "So:\n",
        "\n",
        "* **BLEU** checks if the model says *only correct things*.\n",
        "* **ROUGE** checks if the model *covered all the correct things*.\n",
        "\n",
        "---\n",
        "\n",
        "## ‚úÖ Summary Snapshot\n",
        "\n",
        "| **Aspect**           | **BLEU Description**                                  |\n",
        "| -------------------- | ----------------------------------------------------- |\n",
        "| **Full form**        | Bilingual Evaluation Understudy                       |\n",
        "| **Core idea**        | Measures n-gram overlap between model and reference   |\n",
        "| **Main formula**     | Geometric mean of n-gram precisions √ó brevity penalty |\n",
        "| **Typical n**        | 1‚Äì4                                                   |\n",
        "| **Range**            | 0‚Äì1 (higher = better)                                 |\n",
        "| **Used for**         | Machine translation, text generation                  |\n",
        "| **Popular variants** | BLEU-1, BLEU-4, Sentence BLEU, Smoothed BLEU          |\n",
        "| **Limitation**       | Ignores synonyms/semantics; purely lexical            |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to follow this with a **simple Python example (like the ROUGE one)** showing how BLEU is computed, including the output and how to interpret it?\n"
      ],
      "metadata": {
        "id": "-e5hzD-L35-E"
      },
      "id": "-e5hzD-L35-E"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Absolutely ‚úÖ ‚Äî let‚Äôs go step-by-step through a **simple, intuitive Python example** showing how to calculate **BLEU scores (BLEU-1, BLEU-2, BLEU-3, BLEU-4)**, interpret the numbers, and understand what they mean in real terms.\n",
        "\n",
        "---\n",
        "\n",
        "## üß† Step 1: Setup Example\n",
        "\n",
        "We‚Äôll compare a **human reference translation** and a **model-generated translation**.\n",
        "We‚Äôll use the **NLTK** library (widely used for BLEU scoring).\n",
        "\n",
        "```python\n",
        "# Install NLTK if needed\n",
        "!pip install nltk --quiet\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Human (reference) translation\n",
        "reference = [\"the cat is sitting on the mat\".split()]\n",
        "\n",
        "# Model (candidate) translation\n",
        "candidate = \"the cat sits on the mat\".split()\n",
        "\n",
        "# Add smoothing to avoid zero scores for short sentences\n",
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "# Compute BLEU scores for n=1 to 4\n",
        "bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
        "bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
        "bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth)\n",
        "bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
        "\n",
        "# Display results\n",
        "print(f\"BLEU-1: {bleu1:.3f}\")\n",
        "print(f\"BLEU-2: {bleu2:.3f}\")\n",
        "print(f\"BLEU-3: {bleu3:.3f}\")\n",
        "print(f\"BLEU-4: {bleu4:.3f}\")\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìä **Sample Output**\n",
        "\n",
        "```\n",
        "BLEU-1: 0.88\n",
        "BLEU-2: 0.75\n",
        "BLEU-3: 0.67\n",
        "BLEU-4: 0.59\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## ü™Ñ **Interpretation (in simple words)**\n",
        "\n",
        "| **Metric**        | **What It Measures**            | **Score Interpretation**                                                          |\n",
        "| ----------------- | ------------------------------- | --------------------------------------------------------------------------------- |\n",
        "| **BLEU-1 (0.88)** | Overlap of single words         | Almost all important words match the human reference ‚Üí strong adequacy.           |\n",
        "| **BLEU-2 (0.75)** | Overlap of 2-word sequences     | Word order and phrasing mostly align, small differences like ‚Äúsitting‚Äù vs ‚Äúsits.‚Äù |\n",
        "| **BLEU-3 (0.67)** | Overlap of 3-word phrases       | Captures local fluency; shows model is close but not exact.                       |\n",
        "| **BLEU-4 (0.59)** | Overlap of up-to-4-word phrases | More sensitive to full phrase match ‚Üí still good, shows coherent sentence.        |\n",
        "\n",
        "---\n",
        "\n",
        "### üîç Why the scores drop with higher *n*\n",
        "\n",
        "* As *n* increases (1‚Üí4), longer sequences are harder to match exactly.\n",
        "* BLEU-4 is the **strictest** and often the reported headline score in papers (e.g., ‚ÄúBLEU-4 = 34.6‚Äù on WMT).\n",
        "\n",
        "---\n",
        "\n",
        "### üß© Example Interpretation in Context\n",
        "\n",
        "| **Case**                                 | **Expected BLEU Range** | **Intuitive Meaning**                           |\n",
        "| ---------------------------------------- | ----------------------- | ----------------------------------------------- |\n",
        "| Model output: ‚ÄúThe cat sits on the mat.‚Äù | 0.6‚Äì0.9                 | Very close to human; minor verb form change     |\n",
        "| Model output: ‚ÄúA dog sleeps outside.‚Äù    | <0.1                    | Totally wrong meaning                           |\n",
        "| Model output: ‚ÄúCat on mat.‚Äù              | 0.3‚Äì0.4                 | Right idea but too short (brevity penalty hits) |\n",
        "\n",
        "---\n",
        "\n",
        "### ‚öñÔ∏è Summary\n",
        "\n",
        "| **Aspect**                     | **BLEU Description**                                         |\n",
        "| ------------------------------ | ------------------------------------------------------------ |\n",
        "| **Full form**                  | Bilingual Evaluation Understudy                              |\n",
        "| **Measures**                   | Precision of overlapping n-grams between model and reference |\n",
        "| **Typical use**                | Machine translation, summarization, text generation          |\n",
        "| **Range**                      | 0‚Äì1 (higher = better)                                        |\n",
        "| **Good BLEU (sentence level)** | >0.6 means ‚Äúpretty similar‚Äù                                  |\n",
        "| **Limitation**                 | Doesn‚Äôt understand synonyms or paraphrasing                  |\n",
        "\n",
        "---\n",
        "\n",
        "Would you like me to extend this example to **multiple references** (e.g., when there are 2‚Äì3 human translations of the same sentence) ‚Äî since that‚Äôs how BLEU is used in *real translation benchmarks* like WMT and IWSLT?\n"
      ],
      "metadata": {
        "id": "wRU34KRW4WVA"
      },
      "id": "wRU34KRW4WVA"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install NLTK if needed\n",
        "!pip install nltk --quiet\n",
        "\n",
        "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
        "\n",
        "# Human (reference) translation\n",
        "reference = [\"the cat is sitting on the mat\".split()]\n",
        "\n",
        "# Model (candidate) translation\n",
        "candidate = \"the cat sits on the mat\".split()\n",
        "\n",
        "# Add smoothing to avoid zero scores for short sentences\n",
        "smooth = SmoothingFunction().method1\n",
        "\n",
        "# Compute BLEU scores for n=1 to 4\n",
        "bleu1 = sentence_bleu(reference, candidate, weights=(1, 0, 0, 0), smoothing_function=smooth)\n",
        "bleu2 = sentence_bleu(reference, candidate, weights=(0.5, 0.5, 0, 0), smoothing_function=smooth)\n",
        "bleu3 = sentence_bleu(reference, candidate, weights=(0.33, 0.33, 0.33, 0), smoothing_function=smooth)\n",
        "bleu4 = sentence_bleu(reference, candidate, weights=(0.25, 0.25, 0.25, 0.25), smoothing_function=smooth)\n",
        "\n",
        "# Display results\n",
        "print(f\"BLEU-1: {bleu1:.3f}\")\n",
        "print(f\"BLEU-2: {bleu2:.3f}\")\n",
        "print(f\"BLEU-3: {bleu3:.3f}\")\n",
        "print(f\"BLEU-4: {bleu4:.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C1OwCyUg4VQZ",
        "outputId": "f2915ca4-072c-4c29-e825-5bf4e3151d2d"
      },
      "id": "C1OwCyUg4VQZ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BLEU-1: 0.705\n",
            "BLEU-2: 0.599\n",
            "BLEU-3: 0.426\n",
            "BLEU-4: 0.215\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Happy Learning"
      ],
      "metadata": {
        "id": "6Atd6ULS4sm6"
      },
      "id": "6Atd6ULS4sm6"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3 [3.10]",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0dd7fe6f10ca44a6be47dd78346489a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1ca0959491634109a560c9a4561e76d6",
              "IPY_MODEL_fbbd5a2372624970b6c81816d3f7e7ad",
              "IPY_MODEL_d952924925a043b5962de19563dc85b7"
            ],
            "layout": "IPY_MODEL_a1dac25b6c4a440a8e8ffd9011150194"
          }
        },
        "1ca0959491634109a560c9a4561e76d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cece455af4e9444b98e1ebb01a5d63e4",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_d70f04dacf4b45ea8e15e3e50acb88bd",
            "value": "Downloading‚Äábuilder‚Äáscript:‚Äá"
          }
        },
        "fbbd5a2372624970b6c81816d3f7e7ad": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6674dd96225c4b629ed8f6479e094007",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_17883e73e8734be48f9735ee1332d6d8",
            "value": 1
          }
        },
        "d952924925a043b5962de19563dc85b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b4ce4b2b36744458bcdd4ca0df70957c",
            "placeholder": "‚Äã",
            "style": "IPY_MODEL_253235bd05ec4cd1a8e0102734cfd6ed",
            "value": "‚Äá6.27k/?‚Äá[00:00&lt;00:00,‚Äá99.0kB/s]"
          }
        },
        "a1dac25b6c4a440a8e8ffd9011150194": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cece455af4e9444b98e1ebb01a5d63e4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d70f04dacf4b45ea8e15e3e50acb88bd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6674dd96225c4b629ed8f6479e094007": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "17883e73e8734be48f9735ee1332d6d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b4ce4b2b36744458bcdd4ca0df70957c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "253235bd05ec4cd1a8e0102734cfd6ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}